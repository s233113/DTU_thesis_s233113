{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf23781c",
   "metadata": {},
   "source": [
    "First, generate a jsonl file for the place files (with fields ID, document, and title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322a1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Done! Wrote filtered JSONL to place_files_extracted.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "EXCEL_PATH    = 'protocols_place.xlsx'\n",
    "SHEET_NAME    = 'new_data'\n",
    "PLACE_FOLDER  = 'data/included_place_after_place_revision_csv'\n",
    "OUTPUT_JSONL  = 'place_files_extracted.jsonl'\n",
    "\n",
    "def main():\n",
    "    # Read Excel, force ID and csv pairing to strings\n",
    "    df = pd.read_excel(\n",
    "        EXCEL_PATH,\n",
    "        sheet_name=SHEET_NAME,\n",
    "        dtype={'ID': str, 'csv pairing': str}\n",
    "    )\n",
    "\n",
    "    # Keep only rows with a non-empty csv pairing\n",
    "    df = df[df['csv pairing'].notna() & (df['csv pairing'].str.strip() != '')]\n",
    "\n",
    "    with open(OUTPUT_JSONL, 'w', encoding='utf-8') as out_f:\n",
    "        for _, row in df.iterrows():\n",
    "            trial_id = row['ID'].strip()\n",
    "            csv_name = row['csv pairing'].strip()\n",
    "            csv_path = os.path.join(PLACE_FOLDER, csv_name)\n",
    "\n",
    "            if not os.path.isfile(csv_path):\n",
    "                print(f'⚠️  File not found: {csv_path}')\n",
    "                continue\n",
    "\n",
    "            place_df = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "            if not {'Document', 'Title'}.issubset(place_df.columns):\n",
    "                print(f'⚠️  Missing columns in {csv_name}: {place_df.columns.tolist()}')\n",
    "                continue\n",
    "\n",
    "            # Drop rows where BOTH document and title are empty/whitespace\n",
    "            place_df['Document'] = place_df['Document'].fillna('')\n",
    "            place_df['Title']    = place_df['Title'].fillna('')\n",
    "            mask_nonempty = ~(\n",
    "                (place_df['Document'].str.strip() == '') &\n",
    "                (place_df['Title'].str.strip() == '')\n",
    "            )\n",
    "            place_df = place_df[mask_nonempty]\n",
    "\n",
    "            # Build parallel lists\n",
    "            documents = place_df['Document'].tolist()\n",
    "            titles    = place_df['Title'].tolist()\n",
    "\n",
    "            # Write one JSON object per CSV\n",
    "            out_f.write(json.dumps({\n",
    "                'id':        trial_id,\n",
    "                'document': documents,\n",
    "                'title':    titles\n",
    "            }, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f'✅  Done! Wrote filtered JSONL to {OUTPUT_JSONL}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ece368",
   "metadata": {},
   "source": [
    "Now , change the copy of the extracted protocol text jsonl by replacing filename by trial number (ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2945392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabyf\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Loaded 127 title→ID mappings\n",
      "✅ Remapped JSONL written to llm_extraction_with_ids.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "EXCEL_PATH      = 'protocols_place.xlsx'\n",
    "SHEET_NAME      = 'new_data'\n",
    "INPUT_JSONL     = 'llm_extraction.jsonl'\n",
    "OUTPUT_JSONL    = 'llm_extraction_with_ids.jsonl'\n",
    "\n",
    "def build_title_to_id_map():\n",
    "    df = pd.read_excel(\n",
    "        EXCEL_PATH,\n",
    "        sheet_name=SHEET_NAME,\n",
    "        dtype={'Protocol title': str, 'ID': str, 'csv pairing': str}\n",
    "    )\n",
    "    df = df[df['csv pairing'].notna() & (df['csv pairing'].str.strip() != '')]\n",
    "    # normalize to lowercase & strip\n",
    "    df['Protocol title'] = df['Protocol title'].str.strip().str.lower()\n",
    "    df['ID']             = df['ID'].str.strip()\n",
    "    mapping = dict(zip(df['Protocol title'], df['ID']))\n",
    "    print(f'✔️ Loaded {len(mapping)} title→ID mappings')\n",
    "    return mapping\n",
    "\n",
    "def remap_file_names():\n",
    "    title_to_id = build_title_to_id_map()\n",
    "    missing = set()\n",
    "\n",
    "    with open(INPUT_JSONL, 'r', encoding='utf-8') as inp, \\\n",
    "         open(OUTPUT_JSONL, 'w', encoding='utf-8') as outp:\n",
    "\n",
    "        for i, line in enumerate(inp, 1):\n",
    "            record = json.loads(line)\n",
    "            old_title = record.get('filename')\n",
    "            if old_title is None:\n",
    "                outp.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            key = old_title.strip().lower()\n",
    "            new_id = title_to_id.get(key)\n",
    "\n",
    "            if new_id:\n",
    "                record['filename'] = new_id\n",
    "            else:\n",
    "                missing.add(old_title)\n",
    "\n",
    "            outp.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    if missing:\n",
    "        print(f'⚠️ Missing mapping for {len(missing)} titles:')\n",
    "        for t in sorted(missing):\n",
    "            print(f'    • \"{t}\"')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    remap_file_names()\n",
    "    print(f'✅ Remapped JSONL written to {OUTPUT_JSONL}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fb0f2",
   "metadata": {},
   "source": [
    "Finally, create a combined jsonl with id, extracted text, document, title by matching the two jsonl files by id (id in the protocols text is called filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f19ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Loaded 127 protocol texts\n",
      "✅ Wrote llm_combined_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PROTOCOLS_JSONL = 'llm_extraction_with_ids.jsonl'    # has fields 'id' and 'text'\n",
    "PLACES_JSONL    = 'place_files_extracted.jsonl'        # has fields 'id', 'documents', 'titles'\n",
    "OUTPUT_JSONL    = 'llm_combined_data.jsonl'\n",
    "\n",
    "def load_protocol_texts(path):\n",
    "    \"\"\"\n",
    "    Build a map from trial-id (filename) → protocol text.\n",
    "    Expects each line to have 'filename' and 'text'.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            pid = rec.get('filename')\n",
    "            if pid is not None:\n",
    "                mapping[pid] = rec.get('text', '')\n",
    "    print(f'✔️ Loaded {len(mapping)} protocol texts')\n",
    "    return mapping\n",
    "\n",
    "def combine_per_id(proto_map, places_path, out_path):\n",
    "    \"\"\"\n",
    "    For each record in the place-files JSONL, look up the protocol text\n",
    "    and write a single merged record with:\n",
    "      { id, text, document, title }\n",
    "    \"\"\"\n",
    "    with open(places_path, 'r', encoding='utf-8') as inp, \\\n",
    "         open(out_path,      'w', encoding='utf-8') as outp:\n",
    "\n",
    "        for line in inp:\n",
    "            rec = json.loads(line)\n",
    "            pid = rec.get('id')\n",
    "            if pid is None:\n",
    "                continue\n",
    "\n",
    "            text = proto_map.get(pid)\n",
    "            if text is None:\n",
    "                # Skip if no matching protocol text\n",
    "                continue\n",
    "\n",
    "            doc   = rec.get('document', [])\n",
    "            title = rec.get('title',    [])\n",
    "\n",
    "            merged = {\n",
    "                'id':        pid,\n",
    "                'text':      text,\n",
    "                'document': doc,\n",
    "                'title':    title\n",
    "            }\n",
    "            outp.write(json.dumps(merged, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f'✅ Wrote {out_path}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    protocol_map = load_protocol_texts(PROTOCOLS_JSONL)\n",
    "    combine_per_id(protocol_map, PLACES_JSONL, OUTPUT_JSONL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
