{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e240dd1a",
   "metadata": {},
   "source": [
    "Functions for extraction evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import Levenshtein\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    # Replace newlines with spaces and collapse all whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "# FREE TEXT EVALUATION\n",
    "\n",
    "def compute_text_metrics(extracted, gold):\n",
    "    \"\"\"\n",
    "    Compute free-text evaluation metrics:\n",
    "      - Precision, Recall, and F1 (token level)\n",
    "      - Normalized Levenshtein distance (LD_norm)\n",
    "      - Normalized misspelled words ratio (W_norm)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize whitespace in both texts\n",
    "    extracted = normalize_whitespace(extracted)\n",
    "    gold = normalize_whitespace(gold)\n",
    "    \n",
    "    # Tokenize (splitting by whitespace)\n",
    "    tokens_ext = extracted.split()\n",
    "    tokens_gold = gold.split()\n",
    "    \n",
    "    # Compute precision, recall, and F1 score using token counts\n",
    "    count_ext = Counter(tokens_ext)\n",
    "    count_gold = Counter(tokens_gold)\n",
    "    intersection = sum((count_ext & count_gold).values())\n",
    "    precision = intersection / len(tokens_ext) if tokens_ext else 0\n",
    "    recall = intersection / len(tokens_gold) if tokens_gold else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    # Normalized Levenshtein distance\n",
    "    ld = Levenshtein.distance(extracted, gold)\n",
    "    ld_norm = min(ld / len(gold), 1.0) if gold else 1.0\n",
    "    \n",
    "    # Misspelled words: count words in extracted text that are unknown\n",
    "    misspelled = spell.unknown(tokens_ext)\n",
    "    W_norm = len(misspelled) / len(tokens_gold) if tokens_gold else 0\n",
    "    W_norm = min(W_norm,1)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'ld_norm': ld_norm,\n",
    "        'W_norm': W_norm\n",
    "    }\n",
    "\n",
    "\n",
    "# IMAGE EVALUATION\n",
    "\n",
    "def compute_image_penalty(expected_images, extracted_images=0):\n",
    "    \"\"\"\n",
    "    Compute a simple image penalty based on the number of expected images minus\n",
    "    the number of extracted images.\n",
    "    \"\"\"\n",
    "    penalty = max(expected_images - extracted_images, 0)\n",
    "    return penalty\n",
    "\n",
    "# Table Markdown Parsing Function\n",
    "def markdown_to_df(md):\n",
    "    \"\"\"\n",
    "    Parse a markdown table string into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Split by lines and ignore empty lines.\n",
    "    lines = [line.strip() for line in md.splitlines() if line.strip()]\n",
    "    if len(lines) < 2:\n",
    "        return pd.DataFrame()\n",
    "    header_line = lines[0]\n",
    "    headers = [h.strip() for h in header_line.strip('|').split('|')]\n",
    "    data_rows = []\n",
    "    for line in lines[2:]:\n",
    "        # Split by pipe and strip whitespace.\n",
    "        row = [cell.strip() for cell in line.strip('|').split('|')]\n",
    "        if row:\n",
    "            data_rows.append(row)\n",
    "    return pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "# TABLE EVALUATION FOR MARKDOWN TABLES\n",
    "def compute_table_metrics_markdown(extracted_md, gold_md):\n",
    "    \"\"\"\n",
    "    Compute metrics for a markdown table.\n",
    "    If no table is detected (i.e. the extracted markdown is empty or yields an empty DataFrame),\n",
    "    return a dictionary with default values and a detected flag of 0.\n",
    "    Otherwise, compute the usual metrics and set detected to 1.\n",
    "    \"\"\"\n",
    "    df_ext = markdown_to_df(extracted_md)\n",
    "    df_gold = markdown_to_df(gold_md)\n",
    "    \n",
    "    if df_ext.empty and df_gold.empty:  # Both tables are missing\n",
    "        return {\n",
    "            'row_score': 1,  # Perfect score since both are absent\n",
    "            'col_score': 1,  # Perfect score since both are absent\n",
    "            'precision_table': 1,  # No false positives\n",
    "            'recall_table': 1,     # No false negatives\n",
    "            'f1_table': 1,         # Since both precision and recall are perfect\n",
    "            'ld_table_norm': 0,    # No changes needed\n",
    "            'W_table_norm': 0,     # No misspellings as there's nothing present\n",
    "            'detected': 1          # Indicate detection of no tables successfully\n",
    "        }\n",
    "\n",
    "    if df_ext.empty:\n",
    "        # Table wasnâ€™t detected.\n",
    "        return {\n",
    "            'row_score': 0,\n",
    "            'col_score': 0,\n",
    "            'precision_table': 0,\n",
    "            'recall_table': 0,\n",
    "            'f1_table': 0,\n",
    "            'ld_table_norm': 1,       # A high penalty value\n",
    "            'W_table_norm': 1,        # Assume worst-case misspelling ratio\n",
    "            'detected': 0\n",
    "        }\n",
    "    \n",
    "    # If the table exists\n",
    "    flat_ext = df_ext.values.flatten().tolist()\n",
    "    flat_gold = df_gold.values.flatten().tolist()\n",
    "    \n",
    "    count_ext = Counter(flat_ext)\n",
    "    count_gold = Counter(flat_gold)\n",
    "    intersection = sum((count_ext & count_gold).values())\n",
    "    precision = intersection / len(flat_ext) if flat_ext else 0\n",
    "    recall = intersection / len(flat_gold) if flat_gold else 0\n",
    "    f1_table = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    # Row and Column Scores\n",
    "    rows_ext, rows_gold = (df_ext.shape[0], df_gold.shape[0])\n",
    "    row_score = 1 - abs(rows_ext - rows_gold) / rows_gold if rows_gold > 0 else 0\n",
    "    cols_ext, cols_gold = (df_ext.shape[1], df_gold.shape[1])\n",
    "    col_score = 1 - abs(cols_ext - cols_gold) / cols_gold if cols_gold > 0 else 0\n",
    "\n",
    "    # Serialize tables \n",
    "    serialized_ext = df_ext.to_csv(index=False)\n",
    "    serialized_gold = df_gold.to_csv(index=False)\n",
    "    ld_table = Levenshtein.distance(serialized_ext, serialized_gold)\n",
    "    ld_table_norm = ld_table / len(serialized_gold) if serialized_gold else 1\n",
    "    ld_table_norm = min(ld_table_norm, 1)\n",
    "\n",
    "    # Misspelled words in table text.\n",
    "    tokens_table_ext = serialized_ext.split()\n",
    "    tokens_table_gold = serialized_gold.split()\n",
    "    misspelled_table = spell.unknown(tokens_table_ext)\n",
    "    W_table_norm = len(misspelled_table) / len(tokens_table_gold) if tokens_table_gold else 1\n",
    "    W_table_norm = min(W_table_norm,1)\n",
    "    result = {\n",
    "        'row_score': max(row_score, 0),\n",
    "        'col_score': max(col_score, 0),\n",
    "        'precision_table': precision,\n",
    "        'recall_table': recall,\n",
    "        'f1_table': f1_table,\n",
    "        'ld_table_norm': ld_table_norm,\n",
    "        'W_table_norm': W_table_norm,\n",
    "        'detected': 1\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# AGGREGATE TABLE SCORES\n",
    "\n",
    "def aggregate_table_scores(table_metrics_list, lambda_weight):\n",
    "    \"\"\"\n",
    "    Aggregate scores from multiple tables.\n",
    "    If a particular table's 'detected' value is 0, then it is treated as missing.\n",
    "    Here, if no table is detected at all, I return TPRES=0.\n",
    "    Otherwise, I calculate the arithmetic mean of the per-table composite scores.\n",
    "    The per-table composite score is defined as:\n",
    "      Score_table = f1_table + row_score + col_score - lambda_weight*(ld_table_norm + W_table_norm)\n",
    "    \"\"\"\n",
    "\n",
    "    total = 0\n",
    "    for metrics in table_metrics_list:\n",
    "        score = (metrics['f1_table'] + metrics['row_score'] + metrics['col_score'] -\n",
    "                 lambda_weight * (metrics['ld_table_norm'] + metrics['W_table_norm']))\n",
    "        print(\"Score for table:\", score)\n",
    "        total += score\n",
    "    avg_score = total / len(table_metrics_list)\n",
    "    TPRES = 1\n",
    "    return avg_score, TPRES\n",
    "\n",
    "\n",
    "\n",
    "# COMPOSITE SECTION SCORE\n",
    "\n",
    "def compute_section_score(text_metrics, table_score, image_penalty, presence, weights, lambda_weight):\n",
    "    \"\"\"\n",
    "    Compute the composite score for a given section by squashing each component (free text,\n",
    "    table, and image penalty) into the range [-1, 1] using tanh, then combining them.\n",
    "    \n",
    "    Raw formula:\n",
    "        raw_section_score = PRESENCE * [ \n",
    "            w_text * (F1_text - lambda*(ld_norm + W_norm)) +\n",
    "            w_table * (TPRES * table_score) -\n",
    "            w_img * (image_penalty)\n",
    "        ]\n",
    "    \n",
    "    Then  squash (normalize) each component and the final score.\n",
    "    \"\"\"\n",
    "    w_text = weights.get('w_text', 1.0)\n",
    "    w_table = weights.get('w_table', 1.0)\n",
    "    w_img = weights.get('w_img', 1.0)\n",
    "    \n",
    "    # Compute raw free text component.\n",
    "    raw_text_component = text_metrics['f1'] - lambda_weight * (text_metrics['ld_norm'] + text_metrics['W_norm'])\n",
    "    norm_text_component = math.tanh(raw_text_component)\n",
    "    print(\"Normalized text component:\", norm_text_component)\n",
    "    # Table component \n",
    "    raw_table_component = table_score\n",
    "    norm_table_component = math.tanh(raw_table_component)\n",
    "    print(\"Normalized table component:\", norm_table_component)\n",
    "    # Squash the image penalty\n",
    "    norm_image_penalty = math.tanh(image_penalty)\n",
    "    print(\"Normalized image penalty:\", norm_image_penalty)\n",
    "    # Combine normalized components\n",
    "    raw_section_score = w_text * norm_text_component + w_table * norm_table_component - w_img * norm_image_penalty\n",
    "    \n",
    "    # Multiply by presence indicator and then squash the overall score.\n",
    "    print(\"Raw section score before presence:\", raw_section_score)\n",
    "    final_section_score = presence * math.tanh(raw_section_score)\n",
    "    print(\"Final section score:\", final_section_score)\n",
    "    return final_section_score\n",
    "\n",
    "# AGGREGATE PROTOCOL SCORE\n",
    "\n",
    "def aggregate_protocol_score(section_scores, section_weights):\n",
    "    \"\"\"\n",
    "    Aggregate the composite scores of the four sections into an overall protocol score.\n",
    "    \"\"\"\n",
    "    total_score = sum(section_weights[sec] * section_scores.get(sec, 0) for sec in section_weights)\n",
    "    return total_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75467009",
   "metadata": {},
   "source": [
    "Template to use the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83209620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gold_free_text = \"\"\n",
    "    extracted_free_text = \"\"    \n",
    "    text_metrics = compute_text_metrics(extracted_free_text, gold_free_text)\n",
    "    \n",
    "    print(\"Text Metrics:\", text_metrics)\n",
    "  \n",
    "    #table 1\n",
    "    gold_table_md = (\"\")\n",
    "    extracted_table_md = (\"\")\n",
    "\n",
    "    table_metrics = compute_table_metrics_markdown(extracted_table_md, gold_table_md)\n",
    "\n",
    "    #table 2\n",
    "    gold_table_md = (\"\")\n",
    "    extracted_table_md = (\"\")\n",
    "\n",
    "\n",
    "    table_metrics_2 = compute_table_metrics_markdown(extracted_table_md, gold_table_md)\n",
    "\n",
    "    table_metrics_list = [table_metrics, table_metrics_2] \n",
    "\n",
    "    #print table metrics\n",
    "    print(\"Table Metrics List:\")\n",
    "    for i, metrics in enumerate(table_metrics_list):\n",
    "        print(f\"Table {i+1} Metrics:\", metrics)\n",
    "\n",
    "    lambda_weight = 0.5  # Equal weighting.\n",
    "    agg_table_score, TPRES = aggregate_table_scores(table_metrics_list, lambda_weight)\n",
    "\n",
    "    print(\"Aggregated Table Score:\", agg_table_score)\n",
    "    \n",
    "    image_penalty = compute_image_penalty(expected_images=0)\n",
    "    \n",
    "    # section is present.\n",
    "    presence = 1\n",
    "    \n",
    "    # Define component weights.\n",
    "    comp_weights = {'w_text': 1, 'w_table': 1, 'w_img': 1}\n",
    "    \n",
    "    # Compute composite section score.\n",
    "    clinical_section_score = compute_section_score(text_metrics, agg_table_score, image_penalty, presence, comp_weights, lambda_weight)\n",
    "    print(\"Section Score:\", clinical_section_score)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
